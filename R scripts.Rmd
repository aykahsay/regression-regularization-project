---
title: "Regression"
author: "STA3010A G Assignment"
date: "2025-10-15"
output: html_document
---


###Introduction

This report explores the California Housing dataset, performs Exploratory Data Analysis (EDA), and builds a Multiple Linear Regression model to predict median house values.

##### Libraries
```{r}
library(recipes)
library(caret)
library(magrittr)

library(magrittr)
library(caret)
library(magrittr)
```



#### Load the Dataset
```{r}
# Load the dataset using the absolute path
housing <- read.csv("C:/Users/Admin/OneDrive - United States International University (USIU)/Documents/USIU_A/FS2025/STA3010/G A/housing.csv")

# View the first few rows of the data
head(housing)

```
#### Inspect Data
```{r}
# Check the structure of the dataset
str(housing)

# View the first few rows of the data
head(housing)

```
#### Handle Missing Values
```{r}
housing <- na.omit(housing)
housing

```
#### Encode Categorical Variable


```{r}
housing <- dummyVars(" ~ .", data = housing) %>%
  predict(newdata = housing) %>%
  as.data.frame()

```

```{r}
dummies <- dummyVars(" ~ .", data = housing)
housing <- predict(dummies, newdata = housing)
housing <- as.data.frame(housing)

```


```{r}
str(housing)

```


```{r}
summary(housing)

```
```{r}
colSums(is.na(housing))

```


```{r}
library(corrplot)
corr_matrix <- cor(housing)
corrplot(corr_matrix, method = "color", tl.cex = 0.7)


```

###### Strongest Relationships (Important for Modeling)

| Variables                                         | Correlation | Meaning                                                                                                           |
| ------------------------------------------------- | ----------- | ----------------------------------------------------------------------------------------------------------------- |
| **median_income ↔ median_house_value**            | **+0.688**  | 💰 Strong positive link — as income rises, house prices rise. This will be your **most important predictor**.     |
| **total_rooms ↔ total_bedrooms**                  | **+0.93**   | 🏠 Very strong correlation — more rooms mean more bedrooms (redundant variable, may cause **multicollinearity**). |
| **population ↔ households**                       | **+0.91**   | 👨‍👩‍👧 Strong link — higher population → more households (also redundant).                                      |
| **ocean_proximityINLAND ↔ median_house_value**    | **–0.485**  | 🏜️ Negative — inland houses are cheaper than coastal ones.                                                       |
| **ocean_proximity<1H OCEAN ↔ median_house_value** | **+0.258**  | 🌊 Positive — homes near the ocean (<1 hour) are more expensive.                                                  |

##### Moderate or Weak Relationships


| Variables                                   | Correlation | Interpretation                                                           |
| ------------------------------------------- | ----------- | ------------------------------------------------------------------------ |
| **housing_median_age ↔ median_house_value** | +0.106      | Older neighborhoods are *slightly* more expensive.                       |
| **longitude ↔ latitude**                    | –0.925      | Strong negative — these two describe location (redundant spatially).     |
| **total_rooms ↔ median_house_value**        | +0.133      | Slightly higher house value with more rooms, but not a strong predictor. |


3️⃣ Collinearity Warnings (Variables too similar)

To prevent overfitting during regularization or regression:

total_rooms, total_bedrooms, households, and population are highly correlated.
👉 Keep only one or two (e.g., total_rooms and population) or standardize before modeling.

longitude and latitude are location proxies — keep both, but be aware they are not independent.

#### Split the Data
```{r}
set.seed(123)  # ensures reproducibility

# Use caret's createDataPartition function
library(caret)

split <- createDataPartition(housing$median_house_value, p = 0.8, list = FALSE)

train_data <- housing[split, ]
test_data  <- housing[-split, ]

```

##### Check the Split
```{r}
nrow(train_data)  
nrow(test_data)   

colnames(train_data)
colnames(test_data)

```
#### Fit the Model on Training Data

```{r}
model_train <- lm(median_house_value ~ ., data = train_data)
summary(model_train)

```

# 🏠 California Housing Price Prediction (Linear Regression in R)

## 📘 Project Overview
This project performs **Exploratory Data Analysis (EDA)** and builds a **Multiple Linear Regression Model** to predict housing prices in California based on various socio-economic and geographic factors.

The analysis uses the **California Housing Dataset** and is implemented entirely in **R** using packages such as `caret`, `ggplot2`, and `dplyr`.

---

## 🧮 1. Model Summary

**Model Formula:**

\[
\text{median\_house\_value} = \beta_0 + \beta_1(\text{longitude}) + \beta_2(\text{latitude}) + \dots
\]

**Dataset used:** `train_data`

---

## 📊 2. Model Fit Summary

| Metric | Value | Description |
|:--------------------------|:-----------|:--------------------------------------------------|
| **Residual Standard Error** | 68,870 | Typical prediction error in house prices |
| **Multiple R-squared** | 0.6441 | Model explains 64.4% of variance in house prices |
| **Adjusted R-squared** | 0.6438 | Adjusted for the number of predictors |
| **F-statistic** | 2463 (p < 2.2e-16) | Model is statistically significant overall |

---

## 🧠 3. Interpretation

- The model fits the data well and explains a large portion of the variation in housing prices.  
- **Median income** and **location variables (longitude, latitude)** are strong predictors.  
- The **Residual Error (~\$68,870)** indicates the average prediction deviation from actual prices.  
- The **F-test** confirms that at least one of the predictors significantly affects house value.

---

## 🔍 4. Exploratory Data Analysis (EDA)
EDA was performed to understand relationships between key variables:
- Correlation matrix between numerical features  
- Distribution plots for income and house values  
- Geographic visualization of house prices (latitude vs longitude)

---
##### Predict on Test Data


```{r}
predictions <- predict(model_train, newdata = test_data)

```

##### Evaluate Model Performance
```{r}
# Calculate RMSE and R-squared
rmse <- sqrt(mean((test_data$median_house_value - predictions)^2))
r2 <- cor(test_data$median_house_value, predictions)^2

cat("RMSE:", rmse, "\nR-squared:", r2)

```
# 🧮 California Housing Linear Regression Results

## 📊 Model Performance

| Metric | Value | Description |
|:------------------|:-----------|:------------------------------------|
| **RMSE** | 67,873.12 | Average prediction error (in USD) |
| **R-squared** | 0.6558641 | Model explains 65.6% of variance in house prices |


### Regularization Techniques 

#### Prepare Data for Regularization
```{r}
# Split data for testing (e.g., 80/20 split)
set.seed(123)
train_idx <- sample(1:nrow(x), 0.8 * nrow(x))
x_train <- x[train_idx, ]
y_train <- y[train_idx]
x_test <- x[-train_idx, ]
y_test <- y[-train_idx]

```
#### Ridge Regression (L2)
```{r}
ridge_model <- cv.glmnet(x_train, y_train, alpha = 0)
plot(ridge_model)

```


##### regularization parameter
```{r}
# 🔍 Find best lambda (regularization parameter)
best_lambda_ridge <- ridge_model$lambda.min
best_lambda_ridge
```
# 🧮 Ridge Regression Result Summary

**Best λ (Lambda):** `7889.342`

## 🧠 Interpretation
- The **λ (lambda)** value controls the strength of the penalty applied to large coefficients.  
- A **larger λ** means stronger regularization — coefficients shrink more toward zero.  
- The chosen λ = **7889.342** provides a good balance between:
  - 🔹 Reducing **overfitting**, and  
  - 🔹 Maintaining **predictive accuracy**.

---
✅ This result indicates that your ridge model is well-regularized and performs effectively on your dataset.

#### Prediction_Ridge

```{r}
ridge_pred <- predict(ridge_model, s = ridge_model$lambda.min, newx = x_test)
```

#### Evaluation Ridgr Regression
```{r}
ridge_rmse <- sqrt(mean((ridge_pred - y_test)^2))
ridge_r2 <- 1 - sum((ridge_pred - y_test)^2) / sum((y_test - mean(y_test))^2)
cat("Ridge Regression:\n")
cat("  RMSE:", ridge_rmse, "\n")
cat("  R-squared:", ridge_r2, "\n\n")
```
##### Ridge Regression Diagnostics
```{r}
library(glmnet)
plot(ridge_model$glmnet.fit, xvar = "lambda")

```

```{r}
coef(ridge_model, s = best_lambda_lasso)
```


#### Laso Regression

```{r}

# Perform Lasso Regression (L1 Regularization)
set.seed(123)
lasso_model <- cv.glmnet(x_train, y_train, alpha = 1)

# Cross-validated Lasso plot with title
plot(lasso_model, main = "Cross-Validation Curve for Lasso Regression")




```


```{r}
# Best lambda
best_lambda_lasso <- lasso_model$lambda.min
best_lambda_lasso
```

# 🧮 Lasso Regression Result Summary

**Best λ (Lambda):** `55.77027`

## 🧠 Interpretation
- The Lasso Regression uses **L1 regularization**, which can shrink some coefficients to **exactly zero**, effectively performing **feature selection**.  
- A λ value of **55.77027** means the model applies a **moderate penalty**:
  - It reduces overfitting.
  - Keeps the most relevant predictors.
  - Simplifies the model by removing less important variables.
- This helps improve **model interpretability** while maintaining strong predictive power.

#### Lasso Prediction

```{r}
# Lasso Regression
lasso_pred <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test)
```


##### Evaluation
```{r}
lasso_rmse <- sqrt(mean((lasso_pred - y_test)^2))
lasso_r2 <- 1 - sum((lasso_pred - y_test)^2) / sum((y_test - mean(y_test))^2)

cat("Lasso Regression:\n")
cat("  RMSE:", lasso_rmse, "\n")
cat("  R-squared:", lasso_r2, "\n")
```
##### Lasso Regression Diagnostics
```{r}
coef(lasso_model, s = best_lambda_lasso)

```




```{r}
# Prepare data
x <- model.matrix(median_house_value ~ ., train_data)[, -1]
y <- train_data$median_house_value

# Create training control
train_control <- trainControl(method = "cv", number = 10)

```


```{r}
# Fit PLS Regression model
set.seed(123)
pls_model <- train(
  x = x,
  y = y,
  method = "pls",
  trControl = train_control,
  tuneLength = 10,
  preProcess = c("center", "scale")
)
# Show best model
print(pls_model)
```


```{r}
# Evaluate on test data
x_test <- model.matrix(median_house_value ~ ., test_data)[, -1]
y_test <- test_data$median_house_value

pls_predictions <- predict(pls_model, newdata = x_test)

```


```{r}
# Compute metrics
pls_rmse <- sqrt(mean((y_test - pls_predictions)^2))
pls_r2 <- cor(y_test, pls_predictions)^2

cat("PLS Regression:\n",
    "  RMSE:", pls_rmse, "\n",
    "  R-squared:", pls_r2, "\n")

```
```{r}
library(caret)
plot(pls_model)

```

```{r}
# If you used caret::train with method = "pls"
pls_coef <- coef(pls_model$finalModel, ncomp = 10)  # ncomp = optimal number of components
pls_coef

```


# 🧮 Regression Model Comparison — California Housing

This project compares **Linear Regression**, **Ridge Regression**, **Lasso Regression**, and **Partial Least Squares (PLS) Regression** for predicting median house values.

---

## 📊 Model Performance Summary

| Model                | RMSE       | R-squared | Interpretation |
|----------------------|-----------|-----------|----------------|
| **Linear Regression** | 67,873.12 | 0.6554   | Baseline model with strong predictive power (~65.5% variance explained). |
| **Ridge Regression**  | 69,274.66 | 0.6399   | Regularization reduces overfitting but slightly lowers predictive accuracy. |
| **Lasso Regression**  | 67,651.67 | 0.6566   | Best overall performance; low RMSE and highest R² among all models. |
| **PLS Regression**    | 67,902.41 | 0.6556   | Reduces multicollinearity while preserving predictive power; slightly below Lasso. |

---

## 🧠 Interpretation

- **Lasso Regression** remains the best performer overall.  
- **PLS Regression** also performs well, balancing dimension reduction and predictive accuracy.  
- **Ridge Regression** trades a small decrease in accuracy for more stable coefficient estimates.  
- **Linear Regression** is a strong baseline without regularization.  

---

✅ **Conclusion:**  
For California housing price prediction, **Lasso Regression** is preferred for highest accuracy, but **PLS Regression** is an excellent alternative when multicollinearity among predictors is a concern.


##### Cross Validation
## Cross-Validated RMSE (Training Set)

The table below shows the **10-fold cross-validated RMSE** for all models on the training data. This helps compare predictive performance and model stability.

```{r}
set.seed(123)
ridge_cv <- cv.glmnet(x, y, alpha = 0, nfolds = 10)
best_lambda_ridge <- ridge_cv$lambda.min
rmse_ridge <- sqrt(min(ridge_cv$cvm))  # CV RMSE

# Lasso CV
lasso_cv <- cv.glmnet(x, y, alpha = 1, nfolds = 10)
best_lambda_lasso <- lasso_cv$lambda.min
rmse_lasso <- sqrt(min(lasso_cv$cvm))  # CV RMSE

# PLS CV
library(caret)
set.seed(123)
pls_model <- train(
  median_house_value ~ ., data = train_data,
  method = "pls",
  preProcess = c("center", "scale"),
  tuneLength = 10,
  trControl = trainControl(method = "cv", number = 10)
)
rmse_pls <- pls_model$results$RMSE[pls_model$results$ncomp == pls_model$bestTune$ncomp]

# Linear regression RMSE (CV)
set.seed(123)
lm_model <- train(
  median_house_value ~ ., data = train_data,
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
)
rmse_lm <- lm_model$results$RMSE

# Combine results
cv_results <- data.frame(
  Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "PLS Regression"),
  CV_RMSE = c(rmse_lm, rmse_ridge, rmse_lasso, rmse_pls)
)

cv_results
```


```{r}
# Predictions on test set
x_test <- model.matrix(median_house_value ~ ., test_data)[, -1]
y_test <- test_data$median_house_value

# Linear Regression
pred_lm <- predict(lm_model, newdata = test_data)

# Ridge
pred_ridge <- predict(ridge_model, s = best_lambda_ridge, newx = x_test)

# Lasso
pred_lasso <- predict(lasso_model, s = best_lambda_lasso, newx = x_test)

# PLS
pred_pls <- predict(pls_model, newdata = test_data)

```


```{r}
library(ggplot2)
plot_df <- data.frame(
  Actual = y_test,
  Linear = as.vector(pred_lm),
  Ridge = as.vector(pred_ridge),
  Lasso = as.vector(pred_lasso),
  PLS = as.vector(pred_pls)
)

```



```{r}
library(tidyr)

# Convert to long format for ggplot
plot_long <- pivot_longer(plot_df, cols = -Actual, names_to = "Model", values_to = "Predicted")

ggplot(plot_long, aes(x = Actual, y = Predicted, color = Model)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
  labs(title = "Actual vs Predicted House Values",
       x = "Actual Median House Value",
       y = "Predicted Median House Value") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

```{r}
ggplot(plot_long, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +
  labs(title = "Actual vs Predicted House Values", x = "Actual", y = "Predicted") +
  facet_wrap(~Model) +
  theme_minimal()

```
```{r}
# Predict on test data
y_pred <- predict(lasso_model, newx = x_test, s = "lambda.min")

# Scatter plot: actual vs predicted
plot(y_test, y_pred, 
     xlab = "Actual Median House Value", 
     ylab = "Predicted Median House Value",
     main = "Lasso Regression: Actual vs Predicted",
     col = "blue", pch = 16)
abline(a = 0, b = 1, col = "red", lwd = 2) # perfect fit line

```

```{r}
plot(lasso_model$glmnet.fit, xvar = "lambda", main = "Lasso Regression: Coefficient Paths")

```


```{r}
# Residuals
residuals_lasso <- y_test - y_pred

# Residuals vs Fitted
plot(y_pred, residuals_lasso,
     xlab = "Fitted Values", ylab = "Residuals",
     main = "Lasso Residuals vs Fitted", pch = 16, col = "darkgreen")
abline(h = 0, col = "red", lwd = 2)

# Normal Q-Q plot
qqnorm(residuals_lasso, main = "Lasso Residual Q-Q Plot")
qqline(residuals_lasso, col = "red", lwd = 2)

```

```



